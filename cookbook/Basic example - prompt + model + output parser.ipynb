{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/expression_language/get_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最基本和最常見的用例是將提示範本和模型連結在一起。為了瞭解它是如何工作的，讓我們創建一個鏈，它接受一個主題並生成一個笑話：\n",
    "%pip install langchain-core langchain-community langchain-openai\n",
    "\n",
    "# 1. Prompt\n",
    "\"\"\"prompt 是 BasePromptTemplate，這意味著它接受範本變數的字典並生成一個 PromptValue .A PromptValue 是已完成提示的包裝器，可以傳遞給 an LLM （將字串作為輸入）或 ChatModel （將一系列消息作為輸入）。它可以與任何一種語言模型類型一起使用，因為它定義了用於生成 BaseMessage s 和生成字串的邏輯。\n",
    "\"\"\"\n",
    "# python\n",
    "prompt_value = prompt.invoke({\"topic\": \"ice cream\"})\n",
    "prompt_value\n",
    "# ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])\n",
    "prompt_value.to_messages()\n",
    "# [HumanMessage(content='tell me a short joke about ice cream')]\n",
    "prompt_value.to_string()\n",
    "# 'Human: tell me a short joke about ice cream'\n",
    "\n",
    "#2. Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'讓我給您講一個關於冰淇淋的短嘻嘻哈：\\n\\n中國傳統話中，有個叫做韓金龍的男孩。他在吃冰淇淋時，突然對自己說：“你知不知道，我是世界上最喜愛冰淇淋的人？”\\n\\n然後，韓金龍回答道：“當然知道！因為你總是在喝完一碗冰淇淋後，就要貼心地問我：‘您有沒有想要再吃一碗？’”'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Local\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic} in traditional chinese\")\n",
    "# model = ChatOpenAI(model=\"gpt-4\")\n",
    "model = ChatOllama(model=\"openhermes\")\n",
    "llm = Ollama(model=\"openhermes\")\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# 請注意這段代碼的這一行，我們使用 LCEL 將不同的元件拼湊成一條鏈\n",
    "# 該 | 符號類似於 unix 管道運算元，它將不同的元件連結在一起，將一個元件的輸出作為輸入饋送到下一個元件。\n",
    "# 在此鏈中，使用者輸入被傳遞到提示範本，然後將提示範本輸出傳遞給模型，然後將模型輸出傳遞給輸出解析器。讓我們單獨看一下每個元件，以真正了解發生了什麼。\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"topic\": \"ice cream\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
