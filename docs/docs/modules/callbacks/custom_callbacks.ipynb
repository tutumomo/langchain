{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9af580",
   "metadata": {},
   "source": [
    "# Custom callback handlers\n",
    "\n",
    "To create a custom callback handler we need to determine the [event(s)](/docs/modules/callbacks/) we want our callback handler to handle as well as what we want our callback handler to do when the event is triggered. Then all we need to do is attach the callback handler to the object either as a constructer callback or a request callback (see [callback types](/docs/modules/callbacks/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428d5e5f",
   "metadata": {},
   "source": [
    "In the example below, we'll implement streaming with a custom handler.\n",
    "\n",
    "In our custom callback handler `MyCustomHandler`, we implement the `on_llm_new_token` to print the token we have just received. We then attach our custom handler to the model object as a constructor callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9e8756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My custom handler, token: \n",
      "My custom handler, token: Why\n",
      "My custom handler, token:  do\n",
      "My custom handler, token:  bears\n",
      "My custom handler, token:  have\n",
      "My custom handler, token:  hairy\n",
      "My custom handler, token:  coats\n",
      "My custom handler, token: ?\n",
      "\n",
      "\n",
      "My custom handler, token: F\n",
      "My custom handler, token: ur\n",
      "My custom handler, token:  protection\n",
      "My custom handler, token: !\n",
      "My custom handler, token: \n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class MyCustomHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        print(f\"My custom handler, token: {token}\")\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"Tell me a joke about {animal}\"])\n",
    "\n",
    "# To enable streaming, we pass in `streaming=True` to the ChatModel constructor\n",
    "# Additionally, we pass in our custom handler as a list to the callbacks parameter\n",
    "model = ChatOpenAI(streaming=True, callbacks=[MyCustomHandler()])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"animal\": \"bears\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
